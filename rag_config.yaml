# RAG (Retrieval-Augmented Generation) Configuration

rag:
  enabled: true
  retrieval_mode: "hybrid"  # dense, sparse, or hybrid
  
  # Vector store configuration
  vector_store:
    backend: "faiss"  # faiss, pinecone, or chromadb
    
    faiss:
      index_type: "IVF"  # Flat, IVF, HNSW
      nlist: 100
      nprobe: 10
      metric: "cosine"
      dimension: 768
      index_path: "vector_stores/faiss_index"
      
    pinecone:
      api_key: "${PINECONE_API_KEY}"
      environment: "us-west1-gcp"
      index_name: "llm-rag-index"
      dimension: 768
      metric: "cosine"
      
    chromadb:
      persist_directory: "vector_stores/chromadb"
      collection_name: "documents"
  
  # Embedding model
  embeddings:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    # Or use: "BAAI/bge-large-en-v1.5", "sentence-transformers/all-mpnet-base-v2"
    batch_size: 32
    normalize_embeddings: true
    device: "cuda"
    
  # Retrieval parameters
  retrieval:
    top_k: 5
    score_threshold: 0.7
    max_tokens_per_doc: 512
    rerank: true
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
  # Chunking strategy
  chunking:
    chunk_size: 512
    chunk_overlap: 50
    separator: "\n\n"
    
  # Document processing
  documents:
    source_paths:
      - "data/knowledge_base/"
    file_types: [".txt", ".pdf", ".md", ".json"]
    metadata_fields: ["source", "timestamp", "category"]
    
  # Generation with RAG
  generation:
    include_sources: true
    max_context_length: 4096
    context_template: |
      Context information:
      {context}
      
      Question: {query}
      
      Answer based on the context above:
